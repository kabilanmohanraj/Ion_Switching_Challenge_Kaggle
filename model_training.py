# -*- coding: utf-8 -*-
"""NN_Batchwise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aV9s99sMiQPk9Ipkfub7uR4l1fNZjd0Z
"""

import math
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, Activation, LSTM, BatchNormalization, TimeDistributed, Conv1D, MaxPooling1D
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras import optimizers
from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical
from tensorflow_addons.metrics import F1Score

from sklearn.preprocessing import MinMaxScaler

from google.colab import drive
drive.mount('/content/drive')

train_df = pd.read_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/train_clean.csv')
test_df = pd.read_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/test_clean.csv')
train_df.head()

"""## Random Forest Model training"""

# 5 models created -- 1s, 1f, 3, 5, 10

# upto 1 cahnnel open (low channel open probability)
rf1s = RandomForestClassifier(n_estimators=1000, max_depth=1)

# upto 1 cahnnel open (high channel open probability)
rf1f = RandomForestClassifier(n_estimators=1000, max_depth=1)

# upto 3 cahnnels open
rf3 = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=4)

# upto 5 cahnnels open
rf5 = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=6)

# upto 10 cahnnels open
rf10 = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=11)

from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import EditedNearestNeighbours

resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='not majority'), random_state=100)

X, y = resample.fit_sample(np.array(train_df['signal'][0:1000000]).reshape(-1, 1), np.array(train_df['open_channels'][0:1000000]).reshape(-1, 1))
rf1s.fit(X, y)

rf1f_X = np.concatenate([train_df['signal'].values[1000000: 1500000], train_df['signal'].values[3000000: 3500000]])
rf1f_y = np.concatenate([train_df['open_channels'].values[1000000: 1500000], train_df['open_channels'].values[3000000: 3500000]])

X, y = resample.fit_sample(np.array(rf1f_X).reshape(-1, 1), np.array(rf1f_y).reshape(-1, 1))
rf1f.fit(X, y)

rf3_X = np.concatenate([train_df['signal'].values[1500000: 2000000], train_df['signal'].values[3500000: 4000000]])
rf3_y = np.concatenate([train_df['open_channels'].values[1500000: 2000000], train_df['open_channels'].values[3500000: 4000000]])

X, y = resample.fit_sample(np.array(rf3_X).reshape(-1, 1), np.array(rf3_y).reshape(-1, 1))
rf3.fit(X, y)

rf5_X = np.concatenate([train_df['signal'].values[2500000: 3000000], train_df['signal'].values[4000000: 4500000]])
rf5_y = np.concatenate([train_df['open_channels'].values[2500000: 3000000], train_df['open_channels'].values[4000000: 4500000]])

X, y = resample.fit_sample(np.array(rf5_X).reshape(-1, 1), np.array(rf5_y).reshape(-1, 1))
rf5.fit(X, y)

rf10_y = np.concatenate([train_df['open_channels'].values[2000000: 2500000], train_df['open_channels'].values[4500000: 5000000]])
rf10_y = pd.Series(rf10_y)
index =np.array([rf10_y[rf10_y == 0].index[0], rf10_y[rf10_y == 0].index[1]])

rf10_X = np.concatenate([train_df['signal'].values[2000000: 2500000], train_df['signal'].values[4500000: 5000000]])
rf10_y = np.concatenate([train_df['open_channels'].values[2000000: 2500000], train_df['open_channels'].values[4500000: 5000000]])
rf10_X = np.delete(rf10_X, index)
rf10_y = np.delete(rf10_y, index)
resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='not majority', n_neighbors=2), random_state=100)
X, y = resample.fit_sample(np.array(rf10_X).reshape(-1, 1), np.array(rf10_y).reshape(-1, 1))
rf10.fit(X, y)

# Predicitons
sub = pd.read_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/sample_submission.csv')

a = 0 # SUBSAMPLE A, Model 1s
sub.iloc[100000*a:100000*(a+1),1] = rf1s.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 1 # SUBSAMPLE B, Model 3
sub.iloc[100000*a:100000*(a+1),1] = rf3.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 2 # SUBSAMPLE C, Model 5
sub.iloc[100000*a:100000*(a+1),1] = rf5.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 3 # SUBSAMPLE D, Model 1s
sub.iloc[100000*a:100000*(a+1),1] = rf1s.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 4 # SUBSAMPLE E, Model 1f
sub.iloc[100000*a:100000*(a+1),1] = rf1f.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 5 # SUBSAMPLE F, Model 10
sub.iloc[100000*a:100000*(a+1),1] = rf10.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 6 # SUBSAMPLE G, Model 5
sub.iloc[100000*a:100000*(a+1),1] = rf5.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 7 # SUBSAMPLE H, Model 10
sub.iloc[100000*a:100000*(a+1),1] = rf10.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 8 # SUBSAMPLE I, Model 1s
sub.iloc[100000*a:100000*(a+1),1] = rf1s.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

a = 9 # SUBSAMPLE J, Model 3
sub.iloc[100000*a:100000*(a+1),1] = rf3.predict(test_df['signal'].values[100000*a:100000*(a+1)].reshape((-1,1)))

 # BATCHES 3 AND 4, Model 1s
sub.iloc[1000000:2000000,1] = rf1s.predict(test_df['signal'].values[1000000:2000000].reshape((-1,1)))

sub.to_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/submission-Random-Forest.csv',index=False,float_format='%.4f')

"""## Neural Network training"""

def step_decay(epoch):
    # Learning rate scheduler object
    initial_lrate = 0.1
    drop = 0.001
    epochs_drop = 3.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate



# Model 1s
maxchannels = 1
model1s = Sequential()
timestep = 1
input_dim = 1
model1s.add(TimeDistributed(Conv1D(filters=128, kernel_size=1, activation='relu'), input_shape=(None, 1, 1)))
model1s.add(TimeDistributed(MaxPooling1D(pool_size=1)))
model1s.add(TimeDistributed(Flatten()))
model1s.add(LSTM(256, activation='relu', return_sequences=True))
model1s.add(BatchNormalization())
model1s.add(Dropout(0.2))
model1s.add(LSTM(256, activation='relu', return_sequences=True))
model1s.add(BatchNormalization())
model1s.add(Dropout(0.2))
model1s.add(LSTM(256, activation='relu'))
model1s.add(BatchNormalization())
model1s.add(Dropout(0.2))
model1s.add(Dense(maxchannels+1))
model1s.add(Activation('softmax'))

# Model 1f
maxchannels = 1
model1f = Sequential()
timestep = 1
input_dim = 1
model1f.add(TimeDistributed(Conv1D(filters=128, kernel_size=1, activation='relu'), input_shape=(None, 1, 1)))
model1f.add(TimeDistributed(MaxPooling1D(pool_size=1)))
model1f.add(TimeDistributed(Flatten()))
model1f.add(LSTM(256, activation='relu', return_sequences=True))
model1f.add(BatchNormalization())
model1f.add(Dropout(0.2))
model1f.add(LSTM(256, activation='relu', return_sequences=True))
model1f.add(BatchNormalization())
model1f.add(Dropout(0.2))
model1f.add(LSTM(256, activation='relu'))
model1f.add(BatchNormalization())
model1f.add(Dropout(0.2))
model1f.add(Dense(maxchannels+1))
model1f.add(Activation('softmax'))
model1f.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=11, average='macro')])
lrate = LearningRateScheduler(step_decay)

# Model 3
maxchannels = 3
model3 = Sequential()
timestep = 1
input_dim = 1
model3.add(TimeDistributed(Conv1D(filters=128, kernel_size=1, activation='relu'), input_shape=(None, 1, 1)))
model3.add(TimeDistributed(MaxPooling1D(pool_size=1)))
model3.add(TimeDistributed(Flatten()))
model3.add(LSTM(256, activation='relu', return_sequences=True))
model3.add(BatchNormalization())
model3.add(Dropout(0.2))
model3.add(LSTM(256, activation='relu', return_sequences=True))
model3.add(BatchNormalization())
model3.add(Dropout(0.2))
model3.add(LSTM(256, activation='relu'))
model3.add(BatchNormalization())
model3.add(Dropout(0.2))
model3.add(Dense(maxchannels+1))
model3.add(Activation('softmax'))
model3.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=11, average='macro')])
lrate = LearningRateScheduler(step_decay)

# Model 5
maxchannels = 5
model5 = Sequential()
timestep = 1
input_dim = 1
model5.add(TimeDistributed(Conv1D(filters=128, kernel_size=1, activation='relu'), input_shape=(None, 1, 1)))
model5.add(TimeDistributed(MaxPooling1D(pool_size=1)))
model5.add(TimeDistributed(Flatten()))
model5.add(LSTM(256, activation='relu', return_sequences=True))
model5.add(BatchNormalization())
model5.add(Dropout(0.2))
model5.add(LSTM(256, activation='relu', return_sequences=True))
model5.add(BatchNormalization())
model5.add(Dropout(0.2))
model5.add(LSTM(256, activation='relu'))
model5.add(BatchNormalization())
model5.add(Dropout(0.2))
model5.add(Dense(maxchannels+1))
model5.add(Activation('softmax'))
model5.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=11, average='macro')])
lrate = LearningRateScheduler(step_decay)

# Model 10
maxchannels = 10
model10 = Sequential()
timestep = 1
input_dim = 1
model10.add(TimeDistributed(Conv1D(filters=128, kernel_size=1, activation='relu'), input_shape=(None, 1, 1)))
model10.add(TimeDistributed(MaxPooling1D(pool_size=1)))
model10.add(TimeDistributed(Flatten()))
model10.add(LSTM(256, activation='relu', return_sequences=True))
model10.add(BatchNormalization())
model10.add(Dropout(0.2))
model10.add(LSTM(256, activation='relu', return_sequences=True))
model10.add(BatchNormalization())
model10.add(Dropout(0.2))
model10.add(LSTM(256, activation='relu'))
model10.add(BatchNormalization())
model10.add(Dropout(0.2))
model10.add(Dense(maxchannels))
model10.add(Activation('softmax'))
model10.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=11, average='macro')])
lrate = LearningRateScheduler(step_decay)

"""### Batchwise resampling and training"""

from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import EditedNearestNeighbours

resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='not majority'), random_state=100)

from sklearn.model_selection import train_test_split

X, y = resample.fit_sample(np.array(train_df['signal'][0:1000000]).reshape(-1, 1), np.array(train_df['open_channels'][0:1000000]).reshape(-1, 1))

a = pd.DataFrame(X, columns=['signal'])
a = a.to_numpy()
X = a.reshape(X.shape[0], 1, 1, 1)

b = pd.DataFrame(y, columns=['open_channels'])
b = b.to_numpy()
y = b.reshape(len(b), 1)
from numpy import array
y = to_categorical(array(y), num_classes=2)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=100)

model1s.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=2, average='macro')])
lrate = LearningRateScheduler(step_decay)
model1s.fit(x=X_train, y=y_train, initial_epoch=0, epochs=4, batch_size=256, callbacks=[lrate], verbose=1, shuffle=False, validation_data=(X_val, y_val))

nn1f_X = np.concatenate([train_df['signal'].values[1000000: 1500000], train_df['signal'].values[3000000: 3500000]])
nn1f_y = np.concatenate([train_df['open_channels'].values[1000000: 1500000], train_df['open_channels'].values[3000000: 3500000]])

X, y = resample.fit_sample(np.array(nn1f_X).reshape(-1, 1), np.array(nn1f_y).reshape(-1, 1))

a = pd.DataFrame(X, columns=['signal'])
a = a.to_numpy()
X = a.reshape(X.shape[0], 1, 1, 1)

b = pd.DataFrame(y, columns=['open_channels'])
b = b.to_numpy()
y = b.reshape(len(b), 1)
from numpy import array
y = to_categorical(array(y), num_classes=2)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=100)

model1f.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=2, average='macro')])
lrate = LearningRateScheduler(step_decay)
model1f.fit(x=X_train, y=y_train, initial_epoch=0, epochs=4, batch_size=256, callbacks=[lrate], verbose=1, shuffle=False, validation_data=(X_val, y_val))

nn3_X = np.concatenate([train_df['signal'].values[1500000: 2000000], train_df['signal'].values[3500000: 4000000]])
nn3_y = np.concatenate([train_df['open_channels'].values[1500000: 2000000], train_df['open_channels'].values[3500000: 4000000]])

X, y = resample.fit_sample(np.array(nn3_X).reshape(-1, 1), np.array(nn3_y).reshape(-1, 1))

a = pd.DataFrame(X, columns=['signal'])
a = a.to_numpy()
X = a.reshape(X.shape[0], 1, 1, 1)

b = pd.DataFrame(y, columns=['open_channels'])
b = b.to_numpy()
y = b.reshape(len(b), 1)
from numpy import array
y = to_categorical(array(y), num_classes=4)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=100)

model3.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=4, average='macro')])
lrate = LearningRateScheduler(step_decay)
model3.fit(x=X_train, y=y_train, initial_epoch=0, epochs=4, batch_size=256, callbacks=[lrate], verbose=1, shuffle=False, validation_data=(X_val, y_val))

nn5_X = np.concatenate([train_df['signal'].values[2500000: 3000000], train_df['signal'].values[4000000: 4500000]])
nn5_y = np.concatenate([train_df['open_channels'].values[2500000: 3000000], train_df['open_channels'].values[4000000: 4500000]])

X, y = resample.fit_sample(np.array(nn5_X).reshape(-1, 1), np.array(nn5_y).reshape(-1, 1))

a = pd.DataFrame(X, columns=['signal'])
a = a.to_numpy()
X = a.reshape(X.shape[0], 1, 1, 1)

b = pd.DataFrame(y, columns=['open_channels'])
b = b.to_numpy()
y = b.reshape(len(b), 1)
from numpy import array
y = to_categorical(array(y), num_classes=6)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=100)

model5.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=6, average='macro')])
lrate = LearningRateScheduler(step_decay)
model5.fit(x=X_train, y=y_train, initial_epoch=0, epochs=4, batch_size=256, callbacks=[lrate], verbose=1, shuffle=False, validation_data=(X_val, y_val))

nn10_y = np.concatenate([train_df['open_channels'].values[2000000: 2500000], train_df['open_channels'].values[4500000: 5000000]])
nn10_y = pd.Series(nn10_y)
index =np.array([nn10_y[nn10_y == 0].index[0], nn10_y[nn10_y == 0].index[1]])

nn10_X = np.concatenate([train_df['signal'].values[2000000: 2500000], train_df['signal'].values[4500000: 5000000]])
nn10_y = np.concatenate([train_df['open_channels'].values[2000000: 2500000], train_df['open_channels'].values[4500000: 5000000]])
nn10_X = np.delete(nn10_X, index)
nn10_y = np.delete(nn10_y, index)
resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='not majority'), random_state=100)
X, y = resample.fit_sample(np.array(nn10_X).reshape(-1, 1), np.array(nn10_y).reshape(-1, 1))

a = pd.DataFrame(X, columns=['signal'])
a = a.to_numpy()
X = a.reshape(X.shape[0], 1, 1, 1)

b = pd.DataFrame(y, columns=['open_channels'])
b = b.to_numpy()
y = b.reshape(len(b), 1)
from numpy import array
y = to_categorical(array(y), num_classes=11)

# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=100)

# model10.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=11, average='macro')])
# lrate = LearningRateScheduler(step_decay)
# model10.fit(x=X_train, y=y_train, initial_epoch=0, epochs=4, batch_size=256, callbacks=[lrate], verbose=1, shuffle=False, validation_data=(X_val, y_val))

# y.shape
# y = to_categorical(array(y))

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=100)

# Model 10
maxchannels = 10
model10 = Sequential()
timestep = 1
input_dim = 1
model10.add(TimeDistributed(Conv1D(filters=128, kernel_size=1, activation='relu'), input_shape=(None, 1, 1)))
model10.add(TimeDistributed(MaxPooling1D(pool_size=1)))
model10.add(TimeDistributed(Flatten()))
model10.add(LSTM(256, activation='relu', return_sequences=True))
model10.add(BatchNormalization())
model10.add(Dropout(0.2))
model10.add(LSTM(256, activation='relu', return_sequences=True))
model10.add(BatchNormalization())
model10.add(Dropout(0.2))
model10.add(LSTM(256, activation='relu'))
model10.add(BatchNormalization())
model10.add(Dropout(0.2))
model10.add(Dense(maxchannels+1))
model10.add(Activation('softmax'))
model10.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False), metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=11, average='macro')])
lrate = LearningRateScheduler(step_decay)
model10.fit(x=X_train, y=y_train, initial_epoch=0, epochs=4, batch_size=256, callbacks=[lrate], verbose=1, shuffle=False, validation_data=(X_val, y_val))

test_df.shape

# Predicitons
sub = pd.read_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/sample_submission.csv')
test_df = pd.read_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/test_clean.csv')
test_df = np.array(test_df['signal']).reshape(test_df['signal'].shape[0], 1, 1, 1)
a = 0 # SUBSAMPLE A, Model 1s
pred_y = model1s.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 1 # SUBSAMPLE B, Model 3
pred_y = model3.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 2 # SUBSAMPLE C, Model 5
pred_y = model5.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 3 # SUBSAMPLE D, Model 1s
pred_y = model1s.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 4 # SUBSAMPLE E, Model 1f
pred_y = model1f.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 5 # SUBSAMPLE F, Model 10
pred_y = model10.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 6 # SUBSAMPLE G, Model 5
pred_y = model5.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 7 # SUBSAMPLE H, Model 10
pred_y = model10.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 8 # SUBSAMPLE I, Model 1s
pred_y = model1s.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

a = 9 # SUBSAMPLE J, Model 3
pred_y = model3.predict(test_df[100000*a:100000*(a+1)], batch_size=256)
decoded_datum = np.empty((100000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[100000*a:100000*(a+1),1] = decoded_datum.astype(int)

 # BATCHES 3 AND 4, Model 1s
pred_y = model1s.predict(test_df[1000000:2000000], batch_size=256)
decoded_datum = np.empty((1000000, 1))

def decode(datum):
    return np.argmax(datum)

for i in range(pred_y.shape[0]):
    datum = pred_y[i]
    decoded_datum[i] = decode(datum)
sub.iloc[1000000:2000000,1] = decoded_datum.astype(int)

sub.to_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/submission-Neural-Network.csv',index=False,float_format='%.4f')

ans = pd.read_csv('/content/drive/My Drive/Ion_Channel/Kaggle_dataset/submission-Neural-Network.csv')
ans['open_channels'].mean()